# Awesome Training Free MLLMs



## Papers



| Year | Title                                    | Venue |                    Paper                     | Code |
| ---- | ---------------------------------------- | :---: | :------------------------------------------: | :--: |
| 2025 | **See What You Are Told: Visual Attention Sink in Large Multimodal Models** | ICLR'25 | [Link](https://arxiv.org/pdf/2503.03321?) |  [Code](https://github.com/seilk/VisAttnSink)  |
| 2025 |  **MLLMs Know Where to Look: Training-Free Perception of Small Visual Details With Multimodal LLMs**            |   ICLR'25    |   [Link](https://arxiv.org/pdf/2502.17422)       |  [Code](https://github.com/saccharomycetes/mllms_know)    |
| 2025 | **Stop Looking for “Important Tokens” in Multimodal Language Models: Duplication Matters More**| Arxiv | [Link](https://arxiv.org/pdf/2502.11494?) | [Code](https://github.com/ZichenWen1/DART)|
| 2025 | **Not All Tokens and Heads Are Equally Important: Dual-Level Attention Intervention for Hallucination Mitigation**| Arxiv | [Link](https://arxiv.org/pdf/2506.12609) | -|
| 2025 | **SparseMM: Head Sparsity Emerges from Visual Concept Responses in MLLMs**| Arxiv | [Link](https://arxiv.org/pdf/2506.05344?) | -|
| 2024 | **ControlMLLM: Training-Free Visual Prompt Learning for Multimodal Large Language Models**| NIPS'24 | [Link](https://proceedings.neurips.cc/paper_files/paper/2024/file/4fd96b997454b5b02698595df70fccaf-Paper-Conference.pdf) | [Code](https://github.com/mrwu-mac/ControlMLLM) |

